{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10796238,"sourceType":"datasetVersion","datasetId":6700393}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-24T09:43:52.638037Z","iopub.execute_input":"2025-02-24T09:43:52.638428Z","iopub.status.idle":"2025-02-24T09:43:52.976635Z","shell.execute_reply.started":"2025-02-24T09:43:52.638399Z","shell.execute_reply":"2025-02-24T09:43:52.975952Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/stories/preprocessed.md\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T09:43:52.977642Z","iopub.execute_input":"2025-02-24T09:43:52.978006Z","iopub.status.idle":"2025-02-24T09:43:59.737650Z","shell.execute_reply.started":"2025-02-24T09:43:52.977983Z","shell.execute_reply":"2025-02-24T09:43:59.736841Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n# os.environ[\"PYTORCH_SDP_DISALLOW_FLASH_ATTENTION\"] = \"1\"\n# os.environ[\"PYTORCH_SDP_DISALLOW_MEM_EFFICIENT_ATTENTION\"] = \"1\"\n# os.environ[\"PYTORCH_SDP_DISALLOW_MATH_FALLBACK\"] = \"1\"\n\n# 1. Force synchronous CUDA operations for better traceback\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# 2. Disable W&B\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# 3. (Optional) Disable Flash / SDPA if you're seeing device-side assert errors\nimport torch\n# torch.backends.cuda.enable_flash_sdp(False)\n# torch.backends.cuda.enable_math_sdp(False)\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\n\nprint(\"Environment variables set up. Next, let's import libraries...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T09:43:59.739561Z","iopub.execute_input":"2025-02-24T09:43:59.739808Z","iopub.status.idle":"2025-02-24T09:44:03.044828Z","shell.execute_reply.started":"2025-02-24T09:43:59.739787Z","shell.execute_reply":"2025-02-24T09:44:03.044081Z"}},"outputs":[{"name":"stdout","text":"Environment variables set up. Next, let's import libraries...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    DataCollatorForLanguageModeling, \n    Trainer, \n    TrainingArguments, \n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import Dataset\n# Disable all forms of SDPA / flash attention:\n# torch.backends.cuda.enable_flash_sdp(False)\n# torch.backends.cuda.enable_math_sdp(False)\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\nprint(\"Imports successful.\")\n\n# Confirm versions:\nimport sys\nprint(\"Python version:\", sys.version)\nprint(\"Torch version:\", torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T09:44:03.046242Z","iopub.execute_input":"2025-02-24T09:44:03.046599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_checkpoint = \"sarvamai/sarvam-1\"\nhf_token = \"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\"\n\nprint(\"Loading tokenizer from:\", model_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(\n    model_checkpoint, \n    token=hf_token, \n    trust_remote_code=True\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\nprint(\"Tokenizer loaded. Vocabulary size:\", tokenizer.vocab_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_8bit=True, \n    llm_int8_enable_fp32_cpu_offload=True\n)\n\nmax_memory = {\n    0: \"3GiB\",    # GPU\n    \"cpu\": \"15GiB\"\n}\n\nprint(\"BitsAndBytes config for 8-bit + CPU offload created.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading model from:\", model_checkpoint)\n\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_checkpoint,\n        token=hf_token,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n        max_memory=max_memory\n    )\n    print(\"Model loaded successfully.\")\nexcept Exception as e:\n    print(\"Error loading model:\", e)\n    raise\n\n# if hasattr(model, \"config\"):\n#     if hasattr(model.config, \"_attn_implementation\"):\n#         model.config._attn_implementation = \"math\"  # or \"math\"\n#     if hasattr(model.config, \"attn_config\"):\n#         model.config.attn_config = {\"attn_impl\": \"math\"}\n\nprint(\"Model loaded. No SDPA kernels should be used now.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Resizing token embeddings to:\", len(tokenizer))\nmodel.resize_token_embeddings(len(tokenizer))\n\nprint(\"Applying LoRA config...\")\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\ntry:\n    model = get_peft_model(model, lora_config)\n    print(\"LoRA applied successfully.\")\nexcept Exception as e:\n    print(\"Error applying LoRA:\", e)\n    raise\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ndef load_and_prepare_data(file_path):\n    \"\"\"\n    Load the preprocessed markdown file and split it into individual stories\n    using the marker \"START_OF_STORY\". Each story becomes one training example.\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        data = f.read()\n    # Split on \"START_OF_STORY\" and remove any empty strings\n    stories = data.split(\"START_OF_STORY\")\n    stories = [story.strip() for story in stories if story.strip()]\n    return stories\n\n\npreprocessed_file = \"/kaggle/input/stories/preprocessed.md\"\nstories = load_and_prepare_data(preprocessed_file)\nprint(f\"Number of stories: {len(stories)}\")\n\n\n\ndataset = Dataset.from_dict({\"text\": stories})\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n\nprint(\"Tokenizing dataset...\")\ntokenized_dataset = dataset.map(\n    tokenize_function, \n    batched=True, \n    remove_columns=[\"text\"]\n)\n\nprint(\"Sample tokenized row:\", tokenized_dataset[0])\n\n#################################################\n# Create Data Collator\n#################################################\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir = \"finetuned_sarvam\"\nprint(\"Output dir:\", output_dir)\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    save_steps=10,\n    logging_steps=5,\n    fp16=False,  # 8-bit means we don't rely on half-precision\n    no_cuda=False,\n    gradient_checkpointing=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer created.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting training...\")\ntry:\n    trainer.train()\n    print(\"Training finished!\")\nexcept Exception as e:\n    print(\"Error during training:\", e)\n    raise\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./my_finetuned_model\")\ntokenizer.save_pretrained(\"./my_finetuned_model\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r my_finetuned_model.zip ./my_finetuned_model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r my_finetuned_model.zip ./my_finetuned_model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}