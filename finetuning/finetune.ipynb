{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set up.\n",
      "Python version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n",
      "Torch version: 2.7.0.dev20250224+cu128\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Imports and Environment Setup\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"Environment variables set up.\")\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Tokenizer and Model Loading Functions\n",
    "def load_tokenizer(model_checkpoint, hf_token):\n",
    "    print(\"Loading tokenizer from:\", model_checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Tokenizer loaded. Vocabulary size:\", tokenizer.vocab_size)\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(model_checkpoint, hf_token):\n",
    "    print(\"Loading model from:\", model_checkpoint)\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            token=hf_token,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading model:\", e)\n",
    "        raise\n",
    "    print(\"Model loaded in full precision.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: LoRA Application Function\n",
    "def apply_lora(model, tokenizer):\n",
    "    print(\"Resizing token embeddings to:\", len(tokenizer))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    print(\"Applying LoRA config...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    try:\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print(\"LoRA applied successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error applying LoRA:\", e)\n",
    "        raise\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Data Loading and Tokenization Functions\n",
    "def load_and_prepare_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    stories = data.split(\"START_OF_STORY\")\n",
    "    stories = [story.strip() for story in stories if story.strip()]\n",
    "    return stories\n",
    "\n",
    "def tokenize_dataset(tokenizer, stories):\n",
    "    dataset = Dataset.from_dict({\"text\": stories})\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    print(\"Sample tokenized row:\", tokenized_dataset[0])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: sarvamai/sarvam-1\n",
      "Tokenizer loaded. Vocabulary size: 68096\n",
      "Loading model from: sarvamai/sarvam-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce5ee9ac711477583808e1e968cde63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Model loaded in full precision.\n",
      "Resizing token embeddings to: 68097\n",
      "Applying LoRA config...\n",
      "LoRA applied successfully.\n",
      "Number of stories: 404\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bbd27432814112af239a365f262922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized row: {'input_ids': [1, 4489, 67659, 11799, 4898, 19869, 6842, 16075, 41115, 10747, 16295, 4844, 6996, 50042, 67621, 4489, 36499, 15432, 4373, 31538, 5179, 4578, 7196, 5705, 35845, 11025, 4373, 10072, 7642, 4424, 4373, 66471, 67491, 4615, 5058, 4366, 21618, 4412, 36365, 60150, 8659, 4809, 5705, 4366, 19999, 41161, 8382, 4844, 67494, 67709, 67494, 27067, 13164, 67494, 14732, 9455, 5058, 54509, 6294, 4741, 38188, 4900, 5058, 4373, 6642, 21187, 4408, 5992, 18082, 5956, 5058, 4366, 53374, 4643, 67491, 4427, 4900, 26060, 4412, 4373, 20041, 4408, 22308, 67491, 22452, 6232, 4373, 21040, 7869, 8687, 4424, 6270, 4412, 4366, 24269, 67494, 6879, 5058, 36525, 7196, 10072, 4427, 43366, 5051, 55522, 5548, 67494, 4615, 9177, 8760, 4412, 4366, 8435, 30105, 67481, 5310, 67594, 67484, 4397, 15075, 7237, 4373, 65570, 9183, 8760, 4969, 5992, 11103, 9455, 67491, 4427, 4900, 9183, 4373, 65570, 4489, 67782, 8167, 36694, 4385, 6761, 54509, 6294, 30783, 67491, 4366, 11486, 29253, 25633, 67491, 4689, 4900, 6422, 6642, 11225, 4424, 4373, 9475, 4424, 8470, 4676, 67491, 4427, 10780, 17995, 4676, 5018, 25721, 4408, 34835, 4580, 5100, 4366, 50917, 6756, 10863, 4930, 4654, 4373, 11660, 67491, 4615, 30374, 4689, 54509, 6294, 6948, 6248, 8192, 4424, 5992, 18082, 67494, 12320, 5058, 6393, 6712, 12689, 13677, 10345, 22392, 24296, 5657, 67494, 6879, 6497, 7430, 4676, 5746, 16746, 4412, 4373, 7598, 6145, 17570, 4408, 22789, 5909, 22308, 4786, 5992, 42683, 67594, 56846, 5884, 4427, 8367, 6895, 67491, 33323, 4408, 20811, 4745, 4366, 11660, 5664, 8549, 67491, 4900, 20612, 5746, 4408, 5992, 55522, 5548, 67491, 5432, 5705, 5442, 47911, 67494, 6879, 38239, 4424, 7283, 20811, 4404, 21606, 20923, 9455, 30230, 4578, 6799, 63684, 67491, 9313, 4900, 5058, 5100, 11730, 48173, 23446, 4408, 16648, 4427, 12201, 4424, 4373, 31095, 67594, 67480, 67594, 67520, 13980, 67542, 67594, 4365, 67594, 8708, 67594, 23346, 4628, 27679, 60561, 67494, 5837, 5058, 7966, 60666, 4424, 9973, 9393, 8234, 54509, 6294, 7737, 5746, 66295, 4408, 6775, 19959, 19574, 67491, 4427, 21690, 30200, 29038, 6822, 4408, 34119, 4689, 6497, 13219, 4578, 5171, 5742, 4373, 7642, 67494, 16972, 41938, 5058, 5992, 5612, 4873, 13043, 37755, 67491, 4427, 4373, 12190, 67594, 4370, 5509, 5548, 4427, 28600, 46366, 4408, 4366, 11660, 5664, 4773, 15007, 4745, 55774, 4745, 7823, 4408, 4366, 4417, 20578, 4382, 67494, 54509, 6294, 6422, 12536, 60961, 4412, 16648, 67491, 5674, 5058, 23546, 6480, 4424, 5564, 5940, 4676, 4373, 6712, 21659, 4900, 24076, 4412, 5442, 10894, 8348, 4427, 5442, 10894, 39384, 5411, 67491, 5674, 67491, 5018, 4373, 8176, 6188, 67491, 5058, 7794, 50186, 41275, 4424, 11526, 5746, 7688, 4689, 4900, 31387, 5992, 7676, 5909, 4373, 7642, 4745, 4624, 36280, 8113, 4366, 24269, 5506, 4376, 67652, 4676, 36525, 67594, 67513, 4956, 5171, 4897, 6145, 4366, 14688, 67494, 6879, 37711, 20679, 4489, 67495, 6201, 67621, 4786, 4489, 67492, 6201, 67621, 4427, 4489, 67516, 15075, 67621, 4786, 4489, 67936, 11178, 8382, 4427, 9532, 5203, 4578, 24076, 4689, 5458, 6422, 12536, 5940, 6197, 37711, 20679, 8367, 67494, 4702, 6895, 4530, 5407, 59984, 4412, 5992, 34119, 4900, 33050, 6018, 4786, 4480, 40382, 12165, 4408, 20667, 7404, 4427, 31119, 4427, 22110, 5078, 4676, 67491, 34352, 5442, 4689, 4900, 28442, 4424, 5564, 5940, 25845, 4689, 4900, 9011], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Output directory: finetuned_sarvam\n",
      "Trainer created.\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/101 02:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.491900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshwardhan/Kathakar/gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: my_finetuned_model/ (stored 0%)\n",
      "  adding: my_finetuned_model/added_tokens.json (stored 0%)\n",
      "  adding: my_finetuned_model/adapter_config.json (deflated 54%)\n",
      "  adding: my_finetuned_model/special_tokens_map.json (deflated 78%)\n",
      "  adding: my_finetuned_model/tokenizer_config.json (deflated 97%)\n",
      " (deflated 84%)netuned_model/tokenizer.json\n",
      "  adding: my_finetuned_model/tokenizer.model (deflated 62%)\n",
      " (deflated 53%)netuned_model/adapter_model.safetensors\n",
      "  adding: my_finetuned_model/README.md (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Main Training Function\n",
    "def main():\n",
    "    model_checkpoint = \"sarvamai/sarvam-1\"\n",
    "    hf_token = \"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\"\n",
    "    \n",
    "    tokenizer = load_tokenizer(model_checkpoint, hf_token)\n",
    "    model = load_model(model_checkpoint, hf_token)\n",
    "    model = apply_lora(model, tokenizer)\n",
    "    \n",
    "    preprocessed_file = \"final_dataset.md\"\n",
    "    stories = load_and_prepare_data(preprocessed_file)\n",
    "    print(f\"Number of stories: {len(stories)}\")\n",
    "    \n",
    "    tokenized_dataset = tokenize_dataset(tokenizer, stories)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    \n",
    "    output_dir = \"finetuned_sarvam\"\n",
    "    print(\"Output directory:\", output_dir)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        save_steps=10,\n",
    "        logging_steps=5,\n",
    "        fp16=False,\n",
    "        no_cuda=False,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"Trainer created.\")\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"Training finished!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during training:\", e)\n",
    "        raise\n",
    "    \n",
    "    model.save_pretrained(\"./my_finetuned_model\")\n",
    "    tokenizer.save_pretrained(\"./my_finetuned_model\")\n",
    "    \n",
    "    try:\n",
    "        os.system(\"zip -r my_finetuned_model.zip ./my_finetuned_model\")\n",
    "    except Exception as e:\n",
    "        print(\"Error zipping the model directory:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
